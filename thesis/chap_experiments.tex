%!TEX root = thesis-kdyoung.tex

\chapter{Computational Experiments}
\label{chap:exp}

All experiments were run on a personal computer with
an Intel i5 5575R CPU (four cores each with 2.8GHz clock speed) and 16GB of main memory.
The scripts for the MIPs and the primary script used 
to iterate through the Benders
decomposition algorithm was written in Python 3.4.
We employed Gurobi 7.0.2 as the solver
used for all MIP formulations.
As mentioned previously, \chuffed was the solver
we chose to solve all constraint programs with.
MiniZinc version 2.1.2 was used to compile the models
into the \chuffed FlatZinc format.
Unless stated otherwise, all tests were run
with a time limit of 30 minutes (1800 seconds).

We chose to only test \fsbf{2} and \scbf{2} for the
computational experiments of the previous work.
This is a result of our encoding of the \ssbf{2} model
returning infeasible solutions in our preliminary tests.
Due to time constraints we were unable to discern whether
the problem was our encoding of the model
or if the \ssbf{2} is ill-formed.

We now provide a brief outline of the computational
results reported in this chapter.
First a description of the data used for testing
is given in Section \ref{sec:exp:data}.
As the MIPs presented in \authciteb{Esmaeilbeigi2016}
have been untested, we investigate the best combination
of valid inequalities to include for these models
in Section \ref{sec:exp:mip}.
In Section \ref{sec:exp:cuts} we report on the best combination
of Benders cuts to use for our decomposition.
We provide a detailed investigation into the best configuration
of the CP sub-problem in Section \ref{sec:exp:cp}.
Once the superior sub-problem formulation is determined, 
we close in Section \ref{sec:exp:benchmarkExp} of
this chapter by reporting on the results of our best
solution methodology on a set of benchmark instances.

\section{Data}
\label{sec:exp:data}
The primary dataset available for the \sua{} is
the SBF generated by \citeauthor{Scholl2013}
However, this dataset was created for the type-1 case in
particular and so is not immediately applicable to
the \sua{2}.
The SBF is divided into two halves, the SBF1 and SBF2, 
where each half consists of a total of 1076 instances.
We first provide a brief explanation of how SBF1
was generated.

The basis of the SBF1 are a set of 269 instances
for the \sab{1} containing between 4 and 297 tasks.
When creating instances for the \sua{1}, the cycle time,
task times and precedence relations are all transferred from
the original instances.
We also require values of the forward and backward setup times \ap 
so to create these the authors consider the average processing
time among all tasks, denoted $t_{av}$.
In practice the setup times are smaller than the execution time
of a task, so a parameter $\alpha$ is used when randomly generating
the setups.
An upper bound on the possible setup is defined as $\alpha\cdot t_{av}$
where $\alpha\in\{\:0.25,\:0.50,\:0.75,\:1.00\:\}$.
For example, when $\alpha=1.00$, the largest possible setup time
is 50\% of the average task time.
The 269 input instances are duplicated for each $\alpha$ value
and so 1076 instances of \sua{1} are generated for SBF1.

The SBF2 was generated in almost exactly the same way as SBF1.
However in this case,
care was taken when randomly generating the setup times to ensure
that at least one optimal solution to the original \sab{1} instance
is still feasible in the new \sua{1} instance.
This solution will necessarily be the optimal solution
of the \sua{2} as noted by \citeauthor{Scholl2013}
When storing the instance data, the authors also recorded the
optimal number of stations for the original \sab{1} instance.

The data we chose to run all our tests on was adapted from the SBF2
dataset.
As the optimal solutions were recorded,
we could take the optimal number of stations as input
and then solve for the cycle time.
This simple change allowed 1076 instances for testing our formulations.
\authciteb{Esmaeilbeigi2016} considered a subset of the SBF2 instances
which they divided into 3 classes.
We considered the same subset of instances for the type-2 problem
which are detailed in Table \ref{tab:exp:dataSBF2}.
In this table we give the number of tasks, stations, precedence
relations and order strength of the precedence graph.
In total 396 instances were considered in our tests.

The order strength of an instance is a measure of its complexity
where higher values correspond to more difficult instances.
However, we note that in the extreme, when $OS=1$, there is only one 
precedence-feasible task sequence and thus the problem becomes trivial.

\begin{table}[tpb]
	\centering
	\caption{Dataset summary}
	\vspace{2mm}
	\begin{tabular}{llrrrrr}
		\toprule
		Class & Creator & \#Inst. & $n$ & $m$ & $|E|$ & Order Strength $(OS)$ \\
		\midrule\midrule
		1 &  & 108 & 7-21 & 2-8 & 6-27 & \\[1mm]
		 & {\tt mertens}  & 6 & 7 & 2-6 & 6 & medium (52.40)\\
		  & {\tt bowman8}  & 1 & 8 & 5 & 8 & high (75.00)\\
		  & {\tt jaeschke} & 5 & 9 & 3-8 & 11 & high (83.33)\\
		  & {\tt jackson}  & 6 & 11 & 3-8 & 13 & medium (58.18)\\
		  & {\tt mansoor}  & 3 & 11 & 2-4 & 11 & medium (60.00)\\
		  & {\tt mitchell} & 6 & 21 & 3-8 & 27 & high (70.95)\\\midrule
		2 &  & 112 & 25-30 & 3-14 & 32-40 & \\[1mm]
		  & {\tt roszieg} & 6 & 25 & 4-10 & 32 & high (71.67)\\
		  & {\tt heskia}   & 6 & 28 & 3-8 & 40 & low (22.49)\\
		  & {\tt buxey}    & 7 & 29 & 7-13 & 36 & medium (50.74)\\
		  & {\tt sawyer30} & 9 & 30 & 5-14 & 32 & low (44.83)\\\midrule
		3 &  & 176 & 32-58 & 3-31 & 38-82 & \\[1mm]
		  & {\tt lutz1}   & 6 & 32 & 6-11 & 38 & high (83.47)\\
		  & {\tt gunther}  & 7 & 35 & 7-14 & 43 & medium (59.50)\\
		  & {\tt kilbrid}  & 10 & 45 & 3-10 & 62 & low (44.60)\\
		  & {\tt hahn}     & 5 & 53 & 4-8 & 82 & high (83.82)\\
		  & {\tt warnecke} & 16 & 58 & 14-31 & 70 & medium (59.10)\\\midrule
		Overall & & 396 & 7-58 & 2-31 & 6-82 & \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:dataSBF2}
\end{table}

% \section{Preliminary Tests}
% \label{sec:exp:prelim}

\section{MIP Formulations}
\label{sec:exp:mip}
We first report on how the MIP formulations presented
in Chapter \ref{chap:mip} performed.
In Table \ref{tab:exp:resultsFSBF} and \ref{tab:exp:resultsSCBF} we give the results
of \fsbf{2} and \scbf{2} respectively.
Each is tested on each combination of the valid inequalities from
Section \ref{sec:mip:validIneqs} against
all 3 classes of the subset
of instances from SBF2.
The following datapoints are summarized for each test:
the number of nodes explored by \gurobi's \bab tree,
the average gap for all instances,
the number of instances where no feasible solution was found,
the fraction of instances found to optimality out of the whole class,
the percentage of optimal instances and finally
the average runtime taken by \gurobi.

\begin{table}[tpb]
	\caption{FSBF-2 formulations tested on classes 1,2 and 3}
	\centering
	\vspace{2mm}
	\begin{tabular}{clrrrrrr}
		\toprule
		Class & Ineqs. & \#Nodes & \%Gap & \#No.s. & \#Opt. & \%Opt. & Rt.(s) \\\midrule\midrule
		1 & -- & 40,181 & 0.56 & 0 & 98/108 & 90.74 & 172.90 \\
		 & (\ref{eq:mip:valIneq1}) & 43,437 & 0.16 & 0 & \bf{101}/108 & 93.52 & 166.57 \\
		 & (\ref{eq:mip:valIneq3}) & 50,987 & 1.14 & 0 & 84/108 & 77.78 & 628.47 \\
		 & (\ref{eq:mip:valIneq1}),(\ref{eq:mip:valIneq3}) & 51,734 & 1.12 & 0 & 85/108 & 78.73 & 645.02 \\\midrule
		2 & -- & 171,847 & 14.81 & 0 & 0/112 & 0.00 & 1801.08 \\
		 & (\ref{eq:mip:valIneq1}) & 168,899 & 13.76 & 0 & 0/112 & 0.00 & 1800.66 \\
		 & (\ref{eq:mip:valIneq3}) & 205,752 & 18.04 & 0 & 0/112 & 0.00 & 1801.52 \\
		 & (\ref{eq:mip:valIneq1}),(\ref{eq:mip:valIneq3}) & 202,980 & 17.90 & 0 & 0/112 & 0.00 & 1801.27 \\\midrule
		3 & -- & 48,927 & 19.75 & 9 & 7/176 & 3.98 & 1763.45 \\
		 & (\ref{eq:mip:valIneq1}) & 46,060 & 19.56 & 9 & 7/176 & 3.98 & 1759.67 \\
		 & (\ref{eq:mip:valIneq3}) & 62,845 & 25.64 & 16 & 0/176 & 0.00 & 1800.23 \\
		 & (\ref{eq:mip:valIneq1}),(\ref{eq:mip:valIneq3}) & 63,002 & 24.72 & 15 & 0/176 & 0.00 & 1801.12 \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsFSBF}
\end{table}

\begin{table}[tpb]
	\caption{SCBF-2 formulations tested on classes 1,2 and 3}
	\centering
	\vspace{2mm}
	\begin{tabular}{clrrrrrr}
		\toprule
		Class & Ineqs. & \#Nodes & \%Gap & \#No.s. & \#Opt. & \%Opt. & Rt.(s) \\\midrule\midrule
		1 & -- & 221,842 & 1.62 & 0 & \bf{92}/108 & 86.11 & 374.88 \\
		 & (\ref{eq:mip:valIneq1}) & 244,811 & 1.97 & 0 & 84/108 & 77.78 & 403.17 \\
		 & (\ref{eq:mip:valIneq3}) & 238,515 & 1.75 & 0 & 86/108 & 76.93 & 377.05  \\
		 & (\ref{eq:mip:valIneq1}),(\ref{eq:mip:valIneq3}) & 215,866 & 1.75 & 0 & 86/108 & 76.93 & 380.82  \\\midrule
		2 & -- & 651,880 & 18.01 & 28 & 0/112 & 0.00 & 1800.33 \\
		 & (\ref{eq:mip:valIneq1}) & 701,617 & 19.75 & 31 & 0/112 & 0.00 & 1800.47 \\
		 & (\ref{eq:mip:valIneq3}) & 686,566 & 18.98 & 30 & 0/112 & 0.00 & 1801.03 \\
		 & (\ref{eq:mip:valIneq1}),(\ref{eq:mip:valIneq3}) & 612,442 & 18.57 & 31 & 0/112 & 0.00 & 1800.99 \\\midrule
		3 & -- & 301,055 & 10.09 & 109 & 0/176 & 0.00 & 1800.88 \\
		 & (\ref{eq:mip:valIneq1}) & 326,284 & 12.20 & 117 & 0/176 & 0.00 & 1801.00 \\
		 & (\ref{eq:mip:valIneq3}) & 324,878 & 12.15 & 112 & 0/176 & 0.00 & 1801.21 \\
		 & (\ref{eq:mip:valIneq1}),(\ref{eq:mip:valIneq3}) & 319,908 & 12.06 & 117 & 0/176 & 0.00 & 1801.01 \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsSCBF}
\end{table}

By inspecting the data we find that using the 
valid inequality (\ref{eq:mip:valIneq1})
without (\ref{eq:mip:valIneq3}) is the best 
formulation of \fsbf{2}.
As for \scbf{2}, we found that the best formulation
was when neither of the two valid inequalities were included in
the program.

For both the MIPs, optimality was proven for almost none of the instances of
class 2 and 3.
As a result, all the average runtime values are close
to the time limit of 30 minutes.
In the case of \scbf{2}, no feasible solution was found for 66.5\% of the instances
in class 3.
Even for the smallest class of data, where there is no more
than 21 tasks, neither MIP was able to prove
optimality for all instances.
We note that when calculating the gap value, we only average
the instances where a feasible solution was found.
With a 30 minute time limit, both MIPs could not achieve
an average gap value less than 10\% for class 2 and 3.

Here we tested the full mixed-integer programming approach
on the smallest subset of instances of the SBF2.
The poor results we have found make it clear that these programs
are unsatisfactory exact solution methods for tackling
the \sua{2}.

\section{Cuts}
\label{sec:exp:cuts}
The cutting strategy used in the iteration of the Benders
algorithm can have a large impact on its overall performance.
Even when either the master or sub-problem is exceptionally
difficult to solve, if the Benders cuts added to the master
allow us to find optimality in very few iterations then the problem
can still be tractable.

\subsection{Nogood Cuts}
\label{sec:exp:NGcuts}
In the preliminary testing phase, we considered feasibility sub-problems
using the nogood cuts presented in Section \ref{sec:bend:NGcuts} of the 
previous chapter.
These results are briefly summarized in Table \ref{tab:exp:resultsNGcut}.
Even on the smallest class 1, the Benders
algorithm failed to prove optimality for 66 of the instances tested.
Due to this, we felt that further investigation into the use
of nogood cuts was not the most fruitful research direction. 
Note that we did not make use of the possible parallel computing capabilities
detailed earlier in Section \ref{sec:bend:SPfeas},
where the feasibility sub-problems could be run concurrently.
This potential avenue was not explored as the percentage of computation
time spent solving the sub-problems was dwarfed by the time spent solving
the master problem; as we will see in the coming results.

For all remaining experiments optimality sub-problems were used
unless stated otherwise.

\begin{table}[tpb]
	\caption{Benders algorithm tested on class 1 with nogood cuts}
	\centering
	\vspace{2mm}
	\begin{tabular}{ccrrrr}
		\toprule
		Cuts & Class & \#No sol. & \#Opt. & \%Opt. & Runtime(s) \\\midrule\midrule
		$\mathcal{C}_{ng}$ & 1 & 66 & 42/108 & 38.89 & 1456.57 \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsNGcut}
\end{table}

\subsection{Cutting Procedure}
\label{sec:exp:cuttngProcedure}
In Tables \ref{tab:exp:resultsCutsClass1} and \ref{tab:exp:resultsCutsClass2}
we provide the primary comparison of the possible cutting strategies
that we considered.
The first table compares the 9 possible cut combinations on the smallest
class of data (class 1).
The variation in the results of Table \ref{tab:exp:resultsCutsClass1},
makes it difficult to discern a
clear best cutting procedure.
So we further tested 6 cut combinations
on class 2 in the Table \ref{tab:exp:resultsCutsClass2}.
The choice of solving technology for the sub-problems 
will only effect the time taken to solve all scheduling problems
but not the cutting procedure.
Thus we arbitrarily chose to test all combinations of cuts using MIP
sub-problems.

The two tables comparing the cut combinations contain
the following data:
average number of nodes explored by \gurobi's
\bab tree (total combined between the \rmp{} and sub-problems),
average number iterations of the Benders algorithm,
average number of cuts generated,
average Benders gap,
total number of instances where no feasible solution was found,
percentage of the instances proved optimal and
the average runtime of all instances.

By inspecting the results of Table \ref{tab:exp:resultsCutsClass1}
it is difficult to reason about which cutting strategy is superior
as all instances were proved optimal for every combination.
These results immediately tell us that the Benders decomposition
we present here has outperformed the pure MIP formulations
previously presented by the literature.
We note that when the global bound in not enforced after
a globally feasible solution is found, the Benders gap
remains substantially large and so it is difficult
to measure the convergence of the Benders algorithm.
Thus for the remaining experiments we run, the global bound
will always be used unless stated otherwise.
The $3^{\text{rd}}$ version of the inference cut ($\mathcal{C}_{iii}$) appears to
be a key ingredient in the Benders algorithm terminating efficiently,
but to confirm this suspicion we test our solution
methodology on class 2.

\begin{table}[tpb]
	\caption{Benders cutting strategies compared on class 1}
	\centering
	\vspace{2mm}
	\begin{tabular}{cccccrrrrrrr}
		\toprule
		\multicolumn{5}{c}{Cuts and Bound}  &  &  &  &  &  &  &   \\\cmidrule(rl){1-5}
		$\mathcal{C}_{i}$ & $\mathcal{C}_{ii}$ & $\mathcal{C}_{iii}$ & $\mathcal{C}_{l}$ & $\mathcal{C}_{gb}$& \#Nodes & \#Its. & $|\mathcal{C}|$ & \%Gap & \#No.s. & \%Opt. & Rt.(s) \\\midrule\midrule
		\checkmark &  &  &  &  & 4,718 & 18 & 35 & 0.00 & 0 & 100.00 & 5.8 \\
		 & \checkmark &  &  &  & 2,845 & 15 & 28 & 0.00 & 0 & 100.00 & 4.1 \\
		 &  & \checkmark &  &  & 170 & 4 & 8 & 0.00 & 0 & 100.00 & 1.1 \\\midrule
		\checkmark &  &  &  & \checkmark & 3,790 & 16 & 32 & 0.00 & 0 & 100.00 & 2.9 \\
		 & \checkmark &  &  & \checkmark & 2,064 & 12 & 26 & 0.00 & 0 & 100.00 & 1.9 \\
		 &  & \checkmark &  & \checkmark & 128 & 3 & 9 & 0.00 & 0 & 100.00 & 0.4 \\\midrule
		\checkmark &  &  & \checkmark & \checkmark & 766 & 17 & 34 & 0.00 & 0 & 100.00 & 2.6 \\
		 & \checkmark &  & \checkmark & \checkmark & 896 & 17 & 34 & 0.00 & 0 & 100.00 & 2.8 \\
		 &  & \checkmark & \checkmark & \checkmark & 56 & 4 & 12 & 0.00 & 0 & 100.00 & 0.5 \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsCutsClass1}
\end{table}

In Table \ref{tab:exp:resultsCutsClass2} we can see that
class 2 proved more challenging for the Benders decomposition
to solve the problem as in many cases optimality was
proven for only $\sim30\%$ of the instances.
These results make it clear that $\mathcal{C}_{iii}$
is the strongest infer cut as it is able to
solve almost twice as many instances to optimality
that the other 2 infer cuts.
Further to this, it needed half as many iterations
and similarly half as many cuts in total.
For the instances where the optimal solution was not found,
$\mathcal{C}_{iii}$ provided the lowest Benders gap values
as well.

We must also examine whether including the logic cut, $\mathcal{C}_{l}$,
provided a benefit.
When considering the logic cut, a global bound on the cycle time is instituted
as an upper bound on the load of each station's sub-problem,
possibly leading to feasibility sub-problems, rather than optimality.
The logic cut was formulated to provide the \rmp{} with detailed information
about the assignment variables whilst also reducing the time
needed to solve a sub-problem; however it also has a notable drawback.
When a logic cut is generated by a sub-problem, that sub-problem
has not been solved to optimality.
This results in us not being able to generate an optimality cut for
this sub-problem, such as $\mathcal{C}_{iii}$.
The results of Table \ref{tab:exp:resultsCutsClass2} indicate
that on average, not being able to perform an infer cut has not outweighed the beneficial
information provided by the logic cut.

\begin{table}[tpb]
	\caption{Benders cutting strategies compared on class 2}
	\centering
	\vspace{2mm}
	\begin{tabular}{cccccrrrrrrr}
		\toprule
		\multicolumn{5}{c}{Cuts and Bound}  &  &  &  &  &  &  &   \\\cmidrule(rl){1-5}
		$\mathcal{C}_{i}$ & $\mathcal{C}_{ii}$ & $\mathcal{C}_{iii}$ & $\mathcal{C}_{l}$ & $\mathcal{C}_{gb}$ & \#Nodes & \#Its. & $|\mathcal{C}|$ & \%Gap & \#No.s. & \%Opt. & Rt.(s) \\\midrule\midrule
		\checkmark &  &  &  & \checkmark & 1,522k & 99 & 396 & 11.63 & 1 & 31.25 & 1339.7 \\
		 & \checkmark &  &  & \checkmark & 1,430k & 99 & 399 & 10.79 & 1 & 32.14 & 1348.5 \\
		 &  & \checkmark &  & \checkmark & 1,024k & 47 & 196 & 8.42 & 1 & \bf{60.71} & 959.9 \\\midrule
		\checkmark &  &  & \checkmark & \checkmark & 918k & 112 & 439 & 11.38 & 1 & 33.04 & 1318.4 \\
		 & \checkmark &  & \checkmark & \checkmark & 964k & 114 & 443 & 11.33 & 1 & 31.25 & 1342.3 \\
		 &  & \checkmark & \checkmark & \checkmark & 898k & 68 & 225 & 9.03 & 1 & 58.04 & 956.8 \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsCutsClass2}
\end{table}

Hence, for all remaining experiments we used the infer cut $\mathcal{C}_{iii}$
in combination with the global bound $\mathcal{C}_{gb}$ as the cutting
procedure, unless stated otherwise.

We mention here that combining different 
types of infer cuts together in a single cutting procedure
was not explored.
This is because every inference made by $\mathcal{C}_{i}$ is 
contained within $\mathcal{C}_{ii}$ and every inference made by $\mathcal{C}_{ii}$
is similarly contained within $\mathcal{C}_{iii}$.
Thus combining these infer cuts will not lead to a
substantial reduction in runtime.

\section{CP Sub-Problem}
\label{sec:exp:cp}
The CP solver \chuffed provides the
user with numerous input parameters to
tailor the propagation and search procedure 
to the problem at hand.
In this section, we report on the comparisons
between the model formulation
and search strategies in order to find the best suited
configuration for the \sua{2} instances.

As we are only concerned with optimizing the sub-problems'
solving time, the runtime values we present here will
only average the time used by the CP model, 
\ie excluding the runtime of the master problem.
Similarly, this also applies to the average number of nodes
explored by the CP sub-problems..

% The upper bound cut reduces the space complexity of the sub-problems
% very helpful for cp solver whose difficultly is very closely
% related to the inference that can be done on the finite domains. 

\subsection{Model Formulation}
\label{sec:exp:cpForm}
Two main types of CP models were considered, so we
first check which of these is superior before moving
on to test other input parameters.
$CP_1$ represents the model detailed in Section \ref{sec:bend:cpModel}
from the previous chapter.
$CP_2$ represents the same model but with the additional decision
variable $\sigma_{ij}$ and the \cumu global constraint.

In Table \ref{tab:exp:resultsCPmodel}, we compare these two models 
against each other on the class 1 and
class 2 instances where $\alpha$ is fixed to $1.00$.
The search strategy used for these tests was simply the default
search of \chuffed.
These tests indicate that there is a reduction
in sub-problem runtime when the \cumu constraint
is used in the CP formulation.
For all remaining experiments where CP sub-problems
were used, $CP_2$ was the formulation utilized
unless stated otherwise.

\begin{table}[tpb]
	\caption{CP formulations compared on classes 1 and 2 when $\alpha=1.00$}
	\centering
	\vspace{2mm}
	\begin{tabular}{ccrrrrrr}
		\toprule
		Class & Model & \#SP Nodes & \%Gap & \%Opt. & SP Runtime(s) \\\midrule\midrule
		1 & $CP_1$ & 32,755 & 0.00 & 100.00 & 3.83  \\
		 & $CP_2$ & 29,225 & 0.00 & 100.00 & \bf{2.84} \\\midrule
		2 & $CP_1$ & 2,768,871 & 15.45 & 25.00 & 386.71 \\
		 & $CP_2$ & 2,503,183 & 14.03 & 25.00 & \bf{329.60} \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsCPmodel}
\end{table}

\subsection{Search Strategy}
\label{sec:exp:cpSearch}
Earlier in Section \ref{sec:bend:cpForm}
of the previous Chapter, we detailed a range of possible search procedures
which could be employed when solving our constraint program.
In Table \ref{tab:exp:resultsCPsearch} we present the results
of testing the CP sub-problems on a selection of these procedures.
The names of the searches refer to the following procedures:
{\tt def} is the default search, {\tt start} is the basic search on start time variables,
{\tt start\_ff\_Then\_sigma} is a sequential search of start then $\sigma$ variables. 
Four priority searches were considered which only differ by the variable selection
strategy used to choose the next sequential procedure.
% ; these are 
% {\tt priority\_io} using {\tt input\_order} variable selection, 
% {\tt priority\_sm} using {\tt smallest},
% {\tt priority\_sml} using {\tt smallest\_largest} and
% {\tt priority\_ff} using {\tt first\_fail}.

From the results of Table \ref{tab:exp:resultsCPsearch}, 
we can immediately conclude that the default search
is inferior to the search strategies we have tailored for
the \sua{2}.
Although, among the search procedures we devised, there
is very little variations in the results.
Each procedure was able to find the optimal solution
for the same number of instances,
while the runtime was very similar.
The basic search {\tt start} needed to explore the fewest
number of nodes in the search tree, however the priority search
{\tt priority\_ff} provided the best average runtime.
For all remaining tests where CP sub-problems
were utilized, the priority based search strategy using 
the {\tt first\_fail} variable selection was used.


\begin{table}[tpb]
	\caption{CP search strategies compared on class 2 when $\alpha=1.00$}
	\centering
	\vspace{2mm}
	\begin{tabular}{clrrrrrr}
		\toprule
		Class & Search & \#SP Nodes & \%Gap & \%Opt. & SP Runtime(s) \\\midrule\midrule
		2 & {\tt default}			& 2,503k & 14.03 & 25.00 & 329.60 \\
		 & {\tt start}				& 1,884k & 8.97 & 42.86 & 246.57 \\
		 & {\tt start\_ff\_Then\_sigma}	& 2,391k & 9.69 & 42.86 & 254.23 \\
		 & {\tt priority\_io}		& 2,448k & 9.12 & 42.86 & 253.70 \\
		 & {\tt priority\_sm}		& 2,408k & 9.06 & 42.86 & 252.91 \\
		 & {\tt priority\_sml}		& 2,470k & 9.04 & 42.86 & 252.63 \\
		 & {\tt priority\_ff}		& 2,426k & 8.90 & 42.86 & \bf{245.74} \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsCPsearch}
\end{table}

\section{Benchmark Experiments}
\label{sec:exp:benchmarkExp}
We close this chapter by comparing the best
configuration of our logic-based Benders decomposition
against the results of the MIP formulations for all benchmark
instances from the subset of SBF2 we have used.
Before presenting this final result, we must first
decide on the best sub-problem formulation for our
Benders decomposition.

Earlier in Section \ref{sec:lit:cpChuffed}, we gave a
brief description of the CP solver, \chuffed, we chose to utilize when
tackling the constraint programs we formulated.
As mentioned in that section, \chuffed does not currently have
the ability to be run in parallel.
This handicap has hampered its ability to effectively compete
with \gurobi, which can easily run in parallel on all four cores
of the computer we used for testing.

In Table \ref{tab:exp:resultsSPform} we present a comparison
of our Benders decomposition algorithm tested on the class 2
instances when using MIP and CP sub-problems.
There are many more nodes explored when using CP sub-problems
due to the different search procedure of a CP solver
when compared to a MIP solver.
The results of the two formulations are similar, but we can
conclude that our best configuration of the CP model
has provided a reduction in runtime when compared to the MIP
formulation.

\begin{table}[tpb]
	\caption{MIP and CP sub-problem formulations compared on class 2}
	\centering
	\vspace{2mm}
	\begin{tabular}{ccrrrrrr}
		\toprule
		Class & Sub-Problem & \#SP Nodes & \%Gap & \%Opt. & SP Runtime(s) \\\midrule\midrule
		2 & \spmip{} & 630k & 8.42 & 60.71 & 173.05 \\
		 & \spcp{} & 2,489k & 8.21 & 61.61 & \bf{165.26} \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsSPform}
\end{table}

Now using the best formulation of our Benders decomposition
we present the results of our final benchmark experiments 
on all 3 classes from the subset of the SBF2 instances.
These results can be found in Table \ref{tab:exp:resultsBestBenders}
where we provide the average number of iterations,
cuts, average gap, number optimal and runtime breakdown.
The instances from classes 2 and 3 proved to be more
challenging than the smaller instances from class 1.
Most notably, the total runtime taken by
the Benders algorithm is almost entirely due to the complexity of the \rmp{}.

More cuts are to be expected when the difficultly of the problem
is increased, but we also mention that for the instances of class 3
there is also a larger number of stations.
This will lead to more cuts being made for every iteration of the algorithm.

\begin{table}[tpb]
	\caption{Benchmark results of our final Benders decomposition}
	\centering
	\vspace{2mm}
	\begin{tabular}{crrrrrr}
		\toprule
		Class & \#Its. & \#Cuts. & \%Gap & \#Opt. & RMP.Runtime(s) & SP.Runtime(s) \\\midrule\midrule
		1 & 3 & 9 & 0.00 & 108/108 & 0.36 (87.95\%) & 0.04 (10.98\%) \\
		2 & 47 & 196 & 8.21 & 69/112 & 781.29 (82.33\%) & 165.26 (17.42\%) \\
		3 & 92 & 468 & 15.76 & 27/176 & 1462.01 (88.00\%) & 173.63 (10.45\%) \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsBestBenders}
\end{table}

Table \ref{tab:exp:resultsBenchmark} compares how our final results perform
when compared to the pure MIP formulations of the \sua{2}.
For each class of instances our solution methodology beats the best formulation
of the MIPs.
In the case of class 2 where the Benders decomposition could not prove optimality for
all instances, the average Benders gap at termination is superior to
the best average gap found by \gurobi when solving either \fsbf{2} and \scbf{2}.
For class 3, our algorithm could only find the optimal solution for 15.3\% of the 176 instances.
Also the \scbf{2} model returned superior average gap values for class 3, even though
it was unable to find any optimal solutions.
Although the gap value of \scbf{2} for the class 3 instances
may be misleading, as it only calculated from
the 59 of 176 instances where a feasible solution was found.

\begin{table}[tpb]
	\caption{Benders decomposition compared to pure MIP formulations}
	\centering
	\vspace{2mm}
	\begin{tabular}{crrrrrrrr}
		\toprule
		 & \multicolumn{4}{c}{Benders decomposition}  & \multicolumn{2}{c}{\fsbf{2}} & \multicolumn{2}{c}{\scbf{2}}  \\
		 \cmidrule(lr){2-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
		Class & \%Gap & \#Opt. & \%Opt. & Rt.(s) & \%Gap & \%Opt. & \%Gap & \%Opt. \\\midrule\midrule
		1 & 0.00 & 108/108 & 100.00 & 0.41 & 0.16 & 93.52 & 1.62 & 77.78 \\
		2 & 8.21 & 69/112 & 61.61 & 948.93 & 13.76 & 0.00 & 18.01 & 0.00 \\
		3 & 15.76 & 27/176 & 15.34 & 1661.93 & 19.56 & 3.98 & 10.09 & 0.00 \\
		\bottomrule
	\end{tabular}
	\label{tab:exp:resultsBenchmark}
\end{table}

Our results indicate that the \sua{2} is a difficult problem to solve
when the instance size becomes large.
However, we can see from these results that decomposing the problem using a 
logic-based Benders decomposition framework provides
the most optimal solutions when compared to the other available exact
solution procedures.

% \section{Summary}
% \label{sec:exp:summary}

% blah
